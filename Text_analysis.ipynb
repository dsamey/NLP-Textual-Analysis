{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2cdd67",
   "metadata": {
    "id": "de2cdd67"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c58d37d",
   "metadata": {
    "id": "6c58d37d"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\ameya\\Downloads\\Input.xlsx - Sheet1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a37e0a",
   "metadata": {
    "id": "c5a37e0a",
    "outputId": "0b157c31-9ce3-4ed2-f363-391bcbc4ce10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetch successfully 0\n",
      "Data fetch successfully 1\n",
      "Data fetch successfully 2\n",
      "Data fetch successfully 3\n",
      "Data fetch successfully 4\n",
      "Data fetch successfully 5\n",
      "Data fetch successfully 6\n",
      "URL not found 7\n",
      "Data fetch successfully 8\n",
      "Data fetch successfully 9\n",
      "Data fetch successfully 10\n",
      "Data fetch successfully 11\n",
      "Data fetch successfully 12\n",
      "Data fetch successfully 13\n",
      "Data fetch successfully 14\n",
      "Data fetch successfully 15\n",
      "Data fetch successfully 16\n",
      "Data fetch successfully 17\n",
      "Data fetch successfully 18\n",
      "Data fetch successfully 19\n",
      "URL not found 20\n",
      "Data fetch successfully 21\n",
      "Data fetch successfully 22\n",
      "Data fetch successfully 23\n",
      "Data fetch successfully 24\n",
      "Data fetch successfully 25\n",
      "Data fetch successfully 26\n",
      "Data fetch successfully 27\n",
      "Data fetch successfully 28\n",
      "Data fetch successfully 29\n",
      "Data fetch successfully 30\n",
      "Data fetch successfully 31\n",
      "Data fetch successfully 32\n",
      "Data fetch successfully 33\n",
      "Data fetch successfully 34\n",
      "Data fetch successfully 35\n",
      "Data fetch successfully 36\n",
      "Data fetch successfully 37\n",
      "Data fetch successfully 38\n",
      "Data fetch successfully 39\n",
      "Data fetch successfully 40\n",
      "Data fetch successfully 41\n",
      "Data fetch successfully 42\n",
      "Data fetch successfully 43\n",
      "Data fetch successfully 44\n",
      "Data fetch successfully 45\n",
      "Data fetch successfully 46\n",
      "Data fetch successfully 47\n",
      "Data fetch successfully 48\n",
      "Data fetch successfully 49\n",
      "Data fetch successfully 50\n",
      "Data fetch successfully 51\n",
      "Data fetch successfully 52\n",
      "Data fetch successfully 53\n",
      "Data fetch successfully 54\n",
      "Data fetch successfully 55\n",
      "Data fetch successfully 56\n",
      "Data fetch successfully 57\n",
      "Data fetch successfully 58\n",
      "Data fetch successfully 59\n",
      "Data fetch successfully 60\n",
      "Data fetch successfully 61\n",
      "Data fetch successfully 62\n",
      "Data fetch successfully 63\n",
      "Data fetch successfully 64\n",
      "Data fetch successfully 65\n",
      "Data fetch successfully 66\n",
      "Data fetch successfully 67\n",
      "Data fetch successfully 68\n",
      "Data fetch successfully 69\n",
      "Data fetch successfully 70\n",
      "Data fetch successfully 71\n",
      "Data fetch successfully 72\n",
      "Data fetch successfully 73\n",
      "Data fetch successfully 74\n",
      "Data fetch successfully 75\n",
      "Data fetch successfully 76\n",
      "Data fetch successfully 77\n",
      "Data fetch successfully 78\n",
      "Data fetch successfully 79\n",
      "Data fetch successfully 80\n",
      "Data fetch successfully 81\n",
      "Data fetch successfully 82\n",
      "Data fetch successfully 83\n",
      "Data fetch successfully 84\n",
      "Data fetch successfully 85\n",
      "Data fetch successfully 86\n",
      "Data fetch successfully 87\n",
      "Data fetch successfully 88\n",
      "Data fetch successfully 89\n",
      "Data fetch successfully 90\n",
      "Data fetch successfully 91\n",
      "Data fetch successfully 92\n",
      "Data fetch successfully 93\n",
      "Data fetch successfully 94\n",
      "Data fetch successfully 95\n",
      "Data fetch successfully 96\n",
      "Data fetch successfully 97\n",
      "Data fetch successfully 98\n",
      "Data fetch successfully 99\n",
      "Data fetch successfully 100\n",
      "Data fetch successfully 101\n",
      "Data fetch successfully 102\n",
      "Data fetch successfully 103\n",
      "Data fetch successfully 104\n",
      "Data fetch successfully 105\n",
      "Data fetch successfully 106\n",
      "URL not found 107\n",
      "Data fetch successfully 108\n",
      "Data fetch successfully 109\n",
      "Data fetch successfully 110\n",
      "Data fetch successfully 111\n",
      "Data fetch successfully 112\n",
      "Data fetch successfully 113\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,114):\n",
    "    url = df['URL'][i]\n",
    "    r = requests.get(url, headers={\"User-Agent\": \"XY\"})\n",
    "    if(r.status_code == 200):\n",
    "        print(\"Data fetch successfully\",i)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        title = soup.findAll(attrs = {'class':'entry-title'})\n",
    "        title_1=title[16].text\n",
    "        article = soup.find(attrs = {'class':'td-post-content'})\n",
    "        article_1 = article.text.replace(\"\\n\",\" \")\n",
    "        data = [[title_1, article_1]]\n",
    "        str_data = ' '.join([str(elem) for elem in data])\n",
    "        f = open(str(i)+\"final.txt\", \"a\", encoding = 'utf-8')\n",
    "        f.write(str_data)\n",
    "    else:\n",
    "        print(\"URL not found\",i)\n",
    "\n",
    "f.close()              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d926223",
   "metadata": {
    "id": "0d926223"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'StopWords_Auditor.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-37081b7d691b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                 \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                         \u001b[1;31m# read the data from file1 and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'StopWords_Auditor.txt'"
     ]
    }
   ],
   "source": [
    "\n",
    "filenames = ['StopWords_Auditor.txt', 'StopWords_Currencies.txt', 'StopWords_Names.txt', 'StopWords_DatesandNumbers.txt', 'StopWords_Generic.txt', 'StopWords_GenericLong.txt', 'StopWords_Geographic.txt']\n",
    "\n",
    "\n",
    "with open('Stopwords.txt', 'w') as outfile:\n",
    "\n",
    "\t\n",
    "\tfor names in filenames:\n",
    "\n",
    "\t\t\n",
    "\t\twith open(names) as infile:\n",
    "\n",
    "\t\t\t# read the data from file1 and\n",
    "\t\t\t# file2 and write it in file3\n",
    "\t\t\toutfile.write(infile.read())\n",
    "\n",
    "\t\t# Add '\\n' to enter data of file2\n",
    "\t\t# from next line\n",
    "\t\toutfile.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc346ce",
   "metadata": {
    "id": "8dc346ce"
   },
   "outputs": [],
   "source": [
    "stop_words = open('stopwords.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0441975",
   "metadata": {
    "id": "b0441975"
   },
   "outputs": [],
   "source": [
    "positive = open('positive-words.txt', 'r')\n",
    "pos = positive.read()\n",
    "negative = open('negative-words.txt', 'r')\n",
    "neg = negative.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad26a7",
   "metadata": {
    "id": "c0ad26a7"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "positive_score_total = []\n",
    "negative_score_total = []\n",
    "Polarity_Score = []\n",
    "Avg_sent_length = []\n",
    "Percentage_of_Complex_words = []\n",
    "vowels = ['a','e','i','o','u','A','E','I','O','U']\n",
    "Word_Count = []\n",
    "vow_1 = []\n",
    "Complex_Word_Count = []\n",
    "Personal_Pronouns = []\n",
    "Subjectivity_Score = []\n",
    "PERCENTAGE_OF_COMPLEX_WORDS = []\n",
    "Tot_sent = []\n",
    "syll_per_word = []\n",
    "syll_per_word_1 = []\n",
    "sum_words = []\n",
    "AVG_WORD_LENGTH = []\n",
    "URL = []\n",
    "for i in range(0,114):\n",
    "    cleaned_text = []\n",
    "    complex_words = []\n",
    "    vow = []\n",
    "    try:\n",
    "        text_1 = open(\"Text_files\\\\\"+str(i)+\"final.txt\", 'r', encoding = 'utf-8')\n",
    "        text_2 = open(\"Text_files\\\\\"+str(i)+\"final.txt\", 'r', encoding = 'utf-8')\n",
    "    except:\n",
    "        continue\n",
    "    tokenize_sent = sent_tokenize(text_1.read()) \n",
    "    tokenize_text = word_tokenize(text_2.read())\n",
    "    url = df['URL'][i]\n",
    "    URL.append(url)\n",
    "    p = []\n",
    "    n = []\n",
    "    for w in tokenize_text:\n",
    "        if w in pos:\n",
    "            p.append(w)\n",
    "        elif w in neg:\n",
    "            n.append(w)\n",
    "        elif w not in stopwords:\n",
    "            cleaned_text.append(w)\n",
    "     \n",
    "    positive_score_total.append(len(p)) \n",
    "    negative_score_total.append(len(n))  \n",
    "    Polarity_Score.append((len(p) - len(n))/ ((len(p) + len(n)) + 0.000001))\n",
    "    cleaned_text = [''.join(c for c in s if c not in string.punctuation) for s in cleaned_text]\n",
    "    cleaned_text = [s for s in cleaned_text if s]\n",
    "    word_count = len(cleaned_text)\n",
    "    Avg_sent_length.append(len(cleaned_text)/len(tokenize_sent))\n",
    "    for k in cleaned_text:\n",
    "        temp =  []\n",
    "        for l in k:\n",
    "            \n",
    "            if l in vowels:\n",
    "                temp.append(l)\n",
    "                \n",
    "        vow.append(len(temp))\n",
    "    for m in vow:\n",
    "        if m > 2:\n",
    "            complex_words.append(len(k))\n",
    "    word_len = []\n",
    "    for s in cleaned_text:\n",
    "        word_len.append(len(s))\n",
    "        \n",
    "            \n",
    "            \n",
    "    Complex_Word_Count.append(len(complex_words))        \n",
    "    vow_1.append(len(vow))\n",
    "    Word_Count.append(len(word_len))\n",
    "    syll_2 = [a / b for a, b in zip(vow,word_len)]\n",
    "    syll_per_word.append(sum(syll_2) / len(syll_2)) \n",
    "    sum_words.append(sum(word_len))\n",
    "    AVG_WORD_LENGTH = ([c / d for c, d in zip(sum_words,Word_Count)])\n",
    "    string_lst = ['I', 'we', 'my', 'ours', 'us']\n",
    "    x = re.findall(r\"(?=(\"+'|'.join(string_lst)+r\"))\", ' '.join(cleaned_text))\n",
    "    Personal_Pronouns.append(len(x))\n",
    "    Subjectivity_Score.append((len(p) + len(n))/ ((len(cleaned_text)) + 0.000001))\n",
    "    Tot_sent.append(len(tokenize_sent))\n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384c102",
   "metadata": {
    "id": "9384c102",
    "outputId": "27e3a20a-800f-49c6-81ef-afc38d758ecf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERCENTAGE_OF_COMPLEX_WORDS = res = [q / r for q, r in zip(Complex_Word_Count,Word_Count)]\n",
    "\n",
    "Fog_Index = [0.4*(s + t) for s, t in zip(Avg_sent_length,PERCENTAGE_OF_COMPLEX_WORDS)]\n",
    "Avg_words_per_sent = [u / v for u, v in zip(Word_Count,Tot_sent)]\n",
    "syll_per_word_1 = [ '%.2f' % elem for elem in syll_per_word ]\n",
    "abcd = [c / d for c, d in zip(sum_words,word_len)]\n",
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9357e",
   "metadata": {
    "id": "31a9357e",
    "outputId": "12aa9606-492e-43c2-c2da-f527bf95bcc0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>Personal Pronouns</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>681</td>\n",
       "      <td>124</td>\n",
       "      <td>0.691925</td>\n",
       "      <td>0.887541</td>\n",
       "      <td>12.093333</td>\n",
       "      <td>0.572216</td>\n",
       "      <td>5.066220</td>\n",
       "      <td>12.093333</td>\n",
       "      <td>519</td>\n",
       "      <td>907</td>\n",
       "      <td>0.38</td>\n",
       "      <td>70</td>\n",
       "      <td>7.285557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>708</td>\n",
       "      <td>122</td>\n",
       "      <td>0.706024</td>\n",
       "      <td>1.531365</td>\n",
       "      <td>7.038961</td>\n",
       "      <td>0.453875</td>\n",
       "      <td>2.997134</td>\n",
       "      <td>7.038961</td>\n",
       "      <td>246</td>\n",
       "      <td>542</td>\n",
       "      <td>0.38</td>\n",
       "      <td>52</td>\n",
       "      <td>6.428044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>716</td>\n",
       "      <td>155</td>\n",
       "      <td>0.644087</td>\n",
       "      <td>1.128238</td>\n",
       "      <td>9.190476</td>\n",
       "      <td>0.582902</td>\n",
       "      <td>3.909351</td>\n",
       "      <td>9.190476</td>\n",
       "      <td>450</td>\n",
       "      <td>772</td>\n",
       "      <td>0.39</td>\n",
       "      <td>63</td>\n",
       "      <td>7.222798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>776</td>\n",
       "      <td>123</td>\n",
       "      <td>0.726363</td>\n",
       "      <td>1.335810</td>\n",
       "      <td>7.236559</td>\n",
       "      <td>0.427935</td>\n",
       "      <td>3.065798</td>\n",
       "      <td>7.236559</td>\n",
       "      <td>288</td>\n",
       "      <td>673</td>\n",
       "      <td>0.39</td>\n",
       "      <td>92</td>\n",
       "      <td>6.680535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>787</td>\n",
       "      <td>122</td>\n",
       "      <td>0.731573</td>\n",
       "      <td>1.182055</td>\n",
       "      <td>11.651515</td>\n",
       "      <td>0.466840</td>\n",
       "      <td>4.847342</td>\n",
       "      <td>11.651515</td>\n",
       "      <td>359</td>\n",
       "      <td>769</td>\n",
       "      <td>0.35</td>\n",
       "      <td>73</td>\n",
       "      <td>6.763329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>379</td>\n",
       "      <td>83</td>\n",
       "      <td>0.640693</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>3.622857</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>204</td>\n",
       "      <td>420</td>\n",
       "      <td>0.36</td>\n",
       "      <td>38</td>\n",
       "      <td>7.269048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>634</td>\n",
       "      <td>114</td>\n",
       "      <td>0.695187</td>\n",
       "      <td>0.996005</td>\n",
       "      <td>12.112903</td>\n",
       "      <td>0.483356</td>\n",
       "      <td>5.038504</td>\n",
       "      <td>12.112903</td>\n",
       "      <td>363</td>\n",
       "      <td>751</td>\n",
       "      <td>0.35</td>\n",
       "      <td>59</td>\n",
       "      <td>6.629827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>518</td>\n",
       "      <td>127</td>\n",
       "      <td>0.606202</td>\n",
       "      <td>1.321721</td>\n",
       "      <td>7.870968</td>\n",
       "      <td>0.547131</td>\n",
       "      <td>3.367240</td>\n",
       "      <td>7.870968</td>\n",
       "      <td>267</td>\n",
       "      <td>488</td>\n",
       "      <td>0.38</td>\n",
       "      <td>22</td>\n",
       "      <td>6.721311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>282</td>\n",
       "      <td>39</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.917143</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.625714</td>\n",
       "      <td>5.850286</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>219</td>\n",
       "      <td>350</td>\n",
       "      <td>0.37</td>\n",
       "      <td>25</td>\n",
       "      <td>7.911429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>469</td>\n",
       "      <td>104</td>\n",
       "      <td>0.636998</td>\n",
       "      <td>1.351415</td>\n",
       "      <td>6.424242</td>\n",
       "      <td>0.507075</td>\n",
       "      <td>2.772527</td>\n",
       "      <td>6.424242</td>\n",
       "      <td>215</td>\n",
       "      <td>424</td>\n",
       "      <td>0.39</td>\n",
       "      <td>20</td>\n",
       "      <td>7.148585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  POSITIVE SCORE  \\\n",
       "0    https://insights.blackcoffer.com/ai-in-healthc...             681   \n",
       "1    https://insights.blackcoffer.com/what-if-the-c...             708   \n",
       "2    https://insights.blackcoffer.com/what-jobs-wil...             716   \n",
       "3    https://insights.blackcoffer.com/will-machine-...             776   \n",
       "4    https://insights.blackcoffer.com/will-ai-repla...             787   \n",
       "..                                                 ...             ...   \n",
       "106  https://insights.blackcoffer.com/blockchain-fo...             379   \n",
       "107  https://insights.blackcoffer.com/the-future-of...             634   \n",
       "108  https://insights.blackcoffer.com/big-data-anal...             518   \n",
       "109  https://insights.blackcoffer.com/business-anal...             282   \n",
       "110  https://insights.blackcoffer.com/challenges-an...             469   \n",
       "\n",
       "     NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0               124        0.691925            0.887541            12.093333   \n",
       "1               122        0.706024            1.531365             7.038961   \n",
       "2               155        0.644087            1.128238             9.190476   \n",
       "3               123        0.726363            1.335810             7.236559   \n",
       "4               122        0.731573            1.182055            11.651515   \n",
       "..              ...             ...                 ...                  ...   \n",
       "106              83        0.640693            1.100000             8.571429   \n",
       "107             114        0.695187            0.996005            12.112903   \n",
       "108             127        0.606202            1.321721             7.870968   \n",
       "109              39        0.757009            0.917143            14.000000   \n",
       "110             104        0.636998            1.351415             6.424242   \n",
       "\n",
       "     PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                       0.572216   5.066220                         12.093333   \n",
       "1                       0.453875   2.997134                          7.038961   \n",
       "2                       0.582902   3.909351                          9.190476   \n",
       "3                       0.427935   3.065798                          7.236559   \n",
       "4                       0.466840   4.847342                         11.651515   \n",
       "..                           ...        ...                               ...   \n",
       "106                     0.485714   3.622857                          8.571429   \n",
       "107                     0.483356   5.038504                         12.112903   \n",
       "108                     0.547131   3.367240                          7.870968   \n",
       "109                     0.625714   5.850286                         14.000000   \n",
       "110                     0.507075   2.772527                          6.424242   \n",
       "\n",
       "     COMPLEX WORD COUNT  Word Count SYLLABLE PER WORD  Personal Pronouns  \\\n",
       "0                   519         907              0.38                 70   \n",
       "1                   246         542              0.38                 52   \n",
       "2                   450         772              0.39                 63   \n",
       "3                   288         673              0.39                 92   \n",
       "4                   359         769              0.35                 73   \n",
       "..                  ...         ...               ...                ...   \n",
       "106                 204         420              0.36                 38   \n",
       "107                 363         751              0.35                 59   \n",
       "108                 267         488              0.38                 22   \n",
       "109                 219         350              0.37                 25   \n",
       "110                 215         424              0.39                 20   \n",
       "\n",
       "     AVG WORD LENGTH  \n",
       "0           7.285557  \n",
       "1           6.428044  \n",
       "2           7.222798  \n",
       "3           6.680535  \n",
       "4           6.763329  \n",
       "..               ...  \n",
       "106         7.269048  \n",
       "107         6.629827  \n",
       "108         6.721311  \n",
       "109         7.911429  \n",
       "110         7.148585  \n",
       "\n",
       "[111 rows x 14 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({'URL':URL, 'POSITIVE SCORE':positive_score_total, 'NEGATIVE SCORE':negative_score_total, 'POLARITY SCORE':Polarity_Score,'SUBJECTIVITY SCORE':Subjectivity_Score, 'AVG SENTENCE LENGTH': Avg_sent_length, 'PERCENTAGE OF COMPLEX WORDS':PERCENTAGE_OF_COMPLEX_WORDS, 'FOG INDEX':Fog_Index,'AVG NUMBER OF WORDS PER SENTENCE':Avg_words_per_sent, 'COMPLEX WORD COUNT':Complex_Word_Count, 'Word Count':Word_Count, 'SYLLABLE PER WORD':syll_per_word_1, 'Personal Pronouns':Personal_Pronouns, 'AVG WORD LENGTH':AVG_WORD_LENGTH })\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd14dad",
   "metadata": {
    "id": "8fd14dad"
   },
   "outputs": [],
   "source": [
    "output.to_excel(\"Output_1.xlsx\", index = False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ae1f6",
   "metadata": {
    "id": "575ae1f6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb08ea2",
   "metadata": {
    "id": "fdb08ea2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47bf60",
   "metadata": {
    "id": "ff47bf60"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3793f070",
   "metadata": {
    "id": "3793f070"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fccafc3",
   "metadata": {
    "id": "1fccafc3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d041b87",
   "metadata": {
    "id": "7d041b87"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1c0e19",
   "metadata": {
    "id": "5b1c0e19"
   },
   "outputs": [],
   "source": [
    "\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c955350c",
   "metadata": {
    "id": "c955350c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d350c3",
   "metadata": {
    "id": "d9d350c3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a46ac",
   "metadata": {
    "id": "db6a46ac"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16868ad",
   "metadata": {
    "id": "b16868ad"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4e982",
   "metadata": {
    "id": "c5a4e982"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5bfed",
   "metadata": {
    "id": "1ed5bfed"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87f14fc",
   "metadata": {
    "id": "c87f14fc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddaca90",
   "metadata": {
    "id": "4ddaca90"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25553906",
   "metadata": {
    "id": "25553906"
   },
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235936f7",
   "metadata": {
    "id": "235936f7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a6ade",
   "metadata": {
    "id": "3e7a6ade"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703165b",
   "metadata": {
    "id": "0703165b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f82428d",
   "metadata": {
    "id": "4f82428d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
